# FLM v1.0.0 リリースノート

**リリース日**: 2024年  
**バージョン**: 1.0.0  
**ステータス**: プロダクションレディ

---

## 🎉 FLM v1.0.0 リリースのお知らせ

FLM（Local LLM API Management Tool）の最初の安定版リリースです。

FLMは、**初心者でも外部に公開して使える安全なAPIが、インストールして実行するだけで手に入る**デスクトップアプリケーションです。

技術知識がなくても、コードを書かずに、直感的なUIでローカルLLMのAPIを作成・デプロイし、そのAPIを**外部からも安全に利用**できます。セキュリティ設定は自動化されているため、誰でも簡単に安全なAPIを作成・公開できます。

---

## ✨ 主な機能

### API管理（F001-F003）

- **API作成機能（F001）**
  - 直感的なUIでモデルを選択してAPIを作成
  - 自動でAPIキーを生成・暗号化保存
  - Ollamaの自動起動・確認機能

- **API利用機能（F002）**
  - テスト用チャットインターフェース
  - OpenAI互換APIとして利用可能
  - サンプルコード（curl、Python、JavaScript）の自動生成

- **API管理機能（F003）**
  - APIの起動/停止管理
  - 設定変更（ポート番号、認証設定）
  - API削除とリソースのクリーンアップ

### モデル管理（F004）

- Ollamaモデルの検索・フィルタ・ソート
- モデルダウンロード（進捗表示）
- インストール済みモデル一覧表示
- モデル削除機能

### 認証機能（F005）

- セキュアなAPIキー生成（32文字以上）
- APIキーの暗号化保存
- Bearer Token認証
- APIキー再生成・削除機能

### Ollama自動インストール（F009）

- システムパス上のOllama自動検出
- ポータブル版Ollamaの自動ダウンロード
- 自動起動と状態監視

### 公式Webサイト（F008）

- プロダクト紹介
- ダウンロードページ（OS自動検出）
- 機能紹介、使い方ガイド、FAQ

---

## 📋 システム要件

### 必須要件

- **OS**: Windows 10以降（64bit）
- **メモリ**: 4GB以上（モデルサイズに応じて追加メモリが必要）
- **ストレージ**: 2GB以上の空き容量（モデルダウンロード用の追加容量が必要）
- **Ollama**: 自動インストール対応、または事前インストールが必要

### 推奨要件

- **OS**: Windows 11（64bit）
- **メモリ**: 8GB以上
- **ストレージ**: 10GB以上の空き容量
- **CPU**: 4コア以上

---

## 🚀 インストール方法

詳細なインストール手順は[インストールガイド](./docs/INSTALLATION_GUIDE.md)を参照してください。

### 簡単な手順

1. [公式Webサイト](https://flm.github.io/download)または[GitHub Releases](https://github.com/flm/flm/releases)からインストーラーをダウンロード
2. ダウンロードしたインストーラー（.exe）を実行
3. インストールウィザードに従ってインストール
4. 初回起動時、Ollamaが自動的にインストールされます

---

## 📖 使い方

### 初回起動

1. FLMを起動
2. Ollamaがインストールされていない場合、自動ダウンロードが開始されます
3. ダウンロード完了後、Ollamaが自動起動します

### APIの作成

1. ホーム画面から「新しいAPIを作成」をクリック
2. 使用したいモデルを選択
3. API名とポート番号を設定（オプション）
4. 「作成」ボタンをクリック
5. 作成完了後、APIエンドポイントURLとAPIキーが表示されます

### APIの利用

1. 作成済みAPI一覧からAPIを選択
2. 「テスト」ボタンをクリック
3. チャットインターフェースでメッセージを入力
4. レスポンスが表示されます

詳細な使い方は[ユーザーガイド](./docs/USER_GUIDE.md)を参照してください。

---

## 🐛 既知の問題

### 制限事項

- **Windowsのみ対応**: macOS、Linuxは将来実装予定
- **ログ表示UI未実装**: F006の基盤機能は実装済みですが、UI表示はv1.1で実装予定
- **パフォーマンス監視未実装**: F007はv1.1以降で実装予定

### 既知のバグ

現時点で既知の重大なバグはありません。

問題を発見した場合は、[GitHub Issues](https://github.com/flm/flm/issues)で報告してください。

---

## 🔒 セキュリティ

- APIキーは暗号化（AES-256-GCM）して保存されます
- 認証プロキシでBearer Token認証を実装
- セキュリティヘッダーを設定
- 入力値検証を実装

セキュリティに関する問題を発見した場合は、GitHubのSecurity Advisoriesで報告してください。

---

## 📚 ドキュメント

### ユーザー向け

- [インストールガイド](./docs/INSTALLATION_GUIDE.md) - インストール方法、システム要件
- [ユーザーガイド](./docs/USER_GUIDE.md) - 使い方ガイド
- [FAQ](./docs/FAQ.md) - よくある質問
- [トラブルシューティング](./docs/TROUBLESHOOTING.md) - 問題解決ガイド

### 開発者向け

- [開発者ガイド](./docs/DEVELOPER_GUIDE.md) - アーキテクチャ説明、コントリビューションガイド
- [APIドキュメント](./docs/API_DOCUMENTATION.md) - Tauri IPCコマンドとOpenAI互換API
- [開発環境セットアップ](./docs/DEVELOPMENT_SETUP.md) - 開発環境のセットアップ手順

---

## 🎯 今後の計画

### v1.1（計画中 - 2024年Q2）

- F006: ログ表示機能のUI実装
- F007: パフォーマンス監視機能

### v1.2（計画中 - 2024年Q3）

- モデルカタログ機能の拡張
- バックアップ・復元機能

### v1.3（計画中 - 2024年Q4）

- **複数LLM実行エンジン対応（Phase 1）**: LM Studio対応、抽象化レイヤー実装
- **セキュリティ強化**: IPホワイトリスト、レート制限、APIキーローテーション

### v2.0（将来 - 2025年）

- クロスプラットフォーム対応（macOS、Linux）
- 高度な認証機能（OAuth 2.0等）
- **複数LLM実行エンジン対応（Phase 2）**: vLLM、llama.cpp、text-generation-webui対応

詳細は[将来の拡張計画](./docs/FUTURE_EXTENSIONS.md)を参照してください。

---

## 🙏 謝辞

FLMプロジェクトは以下のオープンソースプロジェクトを使用しています：

- **Tauri** - デスクトップアプリケーションフレームワーク
- **React** - UIライブラリ
- **Rust** - システムプログラミング言語
- **SQLite** - データベース
- **Ollama** - LLM実行環境
- **Express.js** - Webフレームワーク

その他、多くのオープンソースコミュニティに感謝します。

---

## 📄 ライセンス

MIT License

---

## 🔗 リンク

- **公式Webサイト**: https://flm.github.io
- **GitHubリポジトリ**: https://github.com/flm/flm
- **問題報告**: https://github.com/flm/flm/issues
- **機能リクエスト**: https://github.com/flm/flm/issues

---

**FLM v1.0.0 - プロダクションレディ** 🎉

