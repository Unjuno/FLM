# よくある質問（FAQ）

このドキュメントは、FLMアプリケーションに関するよくある質問と回答をまとめています。

---

## 基本事項

### Q1: FLMとは何ですか？

**A**: FLM（Local LLM API Management Tool）は、技術知識がなくても、コードを書かずに、ローカルLLMのAPIを作成・利用できるデスクトップアプリケーションです。

### Q2: FLMは無料ですか？

**A**: はい、FLMは無料で使用できます。オープンソースプロジェクトです。

### Q3: どのOSで動作しますか？

**A**: 現在はWindowsを主にサポートしています。将来的にはmacOS、Linuxにも対応予定です。

---

## インストール・セットアップ

### Q4: FLMのインストール方法は？

**A**: 
1. リリースページからインストーラーをダウンロード
2. インストーラーを実行
3. インストール完了後、アプリケーションを起動

### Q5: 必要なシステム要件は？

**A**:
- **OS**: Windows 10以上（推奨: Windows 11）
- **メモリ**: 8GB以上（推奨: 16GB以上）
- **ストレージ**: 10GB以上の空き容量（モデルによって追加の容量が必要）
- **CPU**: 64ビットプロセッサ

### Q6: Ollamaとは何ですか？

**A**: Ollamaは、ローカルでLLM（大規模言語モデル）を実行するためのツールです。FLMはOllamaを使用してLLMのAPIを提供します。

### Q7: Ollamaのインストールは必要ですか？

**A**: いいえ、必要ありません。FLMが自動的にOllamaをダウンロード・インストールします。既にインストールされている場合は、それを使用します。

---

## APIの作成・利用

### Q8: APIとは何ですか？

**A**: API（Application Programming Interface）は、アプリケーション間でデータをやり取りするためのインターフェースです。FLMで作成したAPIを使用して、他のアプリケーションからLLMの機能を利用できます。

### Q9: どのモデルを選べばいいですか？

**A**: 
- **初心者**: 推奨モデルを選択（画面上でハイライト表示）
- **チャット用途**: llama3:8b、mistral:7bなど
- **コード生成**: codellama:7b、deepseek-coder:6.7bなど
- **軽量モデル**: tinyllama:1.1b、phi:2.7bなど

### Q10: ポート番号は何を選べばいいですか？

**A**: デフォルトの8080で問題ありません。他のアプリケーションが使用している場合は、8081、8082などの別のポート番号を選択してください。

### Q11: 認証を有効にする必要はありますか？

**A**: 
- **ローカル環境のみで使用**: 認証を無効にしても問題ありません
- **外部アプリケーションから使用**: 認証を有効にすることをおすすめします

### Q12: APIキーはどこで確認できますか？

**A**: 
- **API作成時**: 成功画面に表示されます
- **API詳細画面**: 「詳細」ボタンから確認できます
- **設定変更画面**: APIキーを再生成できます

---

## モデル管理

### Q13: モデルのサイズはどのくらいですか？

**A**: モデルによって異なります：
- **小**: 1-3GB（例: tinyllama）
- **中**: 4-7GB（例: llama3:8b）
- **大**: 8GB以上（例: llama3:70b）

### Q14: モデルのダウンロードに時間がかかりますか？

**A**: はい、モデルのサイズとインターネット速度によって異なります：
- **小**: 数分
- **中**: 10-30分
- **大**: 1時間以上

### Q15: モデルを削除できますか？

**A**: はい、インストール済みモデル一覧から削除できます。削除すると、ストレージが解放されます。

---

## トラブルシューティング

### Q16: APIが起動しません

**A**: 以下を確認してください：
1. Ollamaが起動しているか
2. ポート番号が使用されていないか
3. エラーメッセージの内容

### Q17: モデルのダウンロードが失敗します

**A**: 以下を確認してください：
1. インターネット接続
2. 十分なストレージ容量
3. 再試行

### Q18: APIキーを忘れてしまいました

**A**: APIキーを再生成できます。設定変更画面で「APIキーを再生成」ボタンをクリックしてください。古いAPIキーは無効になります。

### Q19: エラーメッセージがわかりません

**A**: FLMのエラーメッセージは非開発者向けに作成されています。メッセージの内容に従って操作してください。問題が解決しない場合は、トラブルシューティングガイドを参照してください。

---

## 技術的な質問

### Q20: FLMはオフラインで動作しますか？

**A**: 
- **モデルダウンロード**: インターネット接続が必要
- **APIの利用**: オフラインで動作します（モデルがダウンロード済みの場合）

### Q21: 複数のAPIを同時に起動できますか？

**A**: はい、異なるポート番号を使用すれば、複数のAPIを同時に起動できます。

### Q22: APIはOpenAI互換ですか？

**A**: はい、OpenAI互換のAPIエンドポイントを提供します。OpenAIのSDKを使用してAPIに接続できます。

### Q23: データはどこに保存されますか？

**A**: 
- **Windows**: `%APPDATA%\FLM\`
- **macOS**: `~/Library/Application Support/FLM/`
- **Linux**: `~/.local/share/FLM/`

---

## セキュリティ

### Q24: APIキーは安全ですか？

**A**: はい、APIキーは暗号化されてデータベースに保存されます。APIキーは作成時のみ表示され、その後は復号化して表示します。

### Q25: データは暗号化されていますか？

**A**: APIキーはAES-256-GCMで暗号化されています。他のデータ（API設定など）は暗号化されていませんが、ローカル環境でのみ使用することを想定しています。

### Q26: SSL証明書について教えてください

**A**: FLMは、API作成時に自動的に自己署名証明書を生成します。この自動生成された証明書で、APIとして問題なく正常に動作します。

- **自動生成**: ユーザー操作不要で自動的に証明書が生成されます
- **APIとして機能**: 自動生成された証明書でAPIとして完全に機能します
- **ブラウザ警告**: 自己署名証明書のため、ブラウザで警告が表示されますが、これは正常です。APIとして機能する上で問題はありません

### Q27: 自分で証明書を設定できますか？

**A**: 現在の実装では、ユーザーが自分で証明書を設定する機能は実装されていません。ただし、自動生成された証明書でAPIとして問題なく正常に動作します。

将来的には、ユーザーが自分で発行した証明書（Let's Encrypt、企業CA等）を使用する機能を追加することを検討していますが、これは必須ではありません。現在の自動生成機能でAPIとして十分に機能します。

---

## パフォーマンス

### Q28: APIの応答速度はどのくらいですか？

**A**: モデルとハードウェアによって異なります：
- **軽量モデル**: 数秒
- **中規模モデル**: 10-30秒
- **大規模モデル**: 1分以上

### Q29: メモリ使用量はどのくらいですか？

**A**: モデルサイズに応じて異なります：
- **小**: 2-4GB
- **中**: 8-12GB
- **大**: 16GB以上

### Q30: CPU使用率が高いです

**A**: LLMの推論処理はCPUを多く使用します。これは正常な動作です。必要に応じて、軽量モデルを使用することをおすすめします。

---

## その他

### Q31: 機能要望やバグ報告はどこでできますか？

**A**: GitHub Issuesで報告してください。

### Q32: 開発に参加したいです

**A**: コントリビューションを歓迎します。開発者ガイドを参照してください。

---

## まだ質問がある場合

- [ユーザーガイド](./USER_GUIDE.md): より詳細な使用方法
- [トラブルシューティングガイド](./TROUBLESHOOTING.md): トラブルシューティング情報
- [GitHub Issues](https://github.com/your-repo/issues): バグ報告・機能要望

