# FLM - 機能仕様書

## 文書情報

- **プロジェクト名**: FLM
- **バージョン**: 1.0.0
- **作成日**: 2024年
- **最終更新日**: 2024年
- **ステータス**: ドラフト

---

## 目次

1. [システム概要](#システム概要)
2. [機能仕様](#機能仕様)
3. [技術仕様](#技術仕様)
4. [UI/UX仕様](#uiux仕様)
5. [API仕様](#api仕様)
6. [セキュリティ仕様](#セキュリティ仕様)
7. [パフォーマンス要件](#パフォーマンス要件)
8. [非機能要件](#非機能要件)
9. [開発・テスト方針](#開発テスト方針)
10. [用語集](#用語集)

---

## システム概要

### 1.1 目的

FLMは、**初心者でも外部に公開して使える安全なAPIが、インストールして実行するだけで手に入る**デスクトップアプリケーションです。

技術知識がなくても、コードを書かずに、直感的なUIでローカルLLMのAPIを作成・デプロイし、そのAPIを外部からも安全に利用できます。セキュリティ設定（APIキー認証など）は自動化されているため、ユーザーは意識せずに安全なAPIを作成・公開できます。

### 1.2 システムアーキテクチャ

```
┌─────────────────────────────────────┐
│     FLM (デスクトップアプリ)    │
│  ┌──────────┐  ┌──────────┐        │
│  │  API作成  │  │  API利用  │        │
│  └────┬─────┘  └────┬─────┘        │
│       │              │               │
│       └──────┬───────┘               │
│              │                        │
│     ┌────────▼────────┐              │
│     │   LLM実行エンジン  │              │
│     │  (Ollama等)     │              │
│     └────────┬────────┘              │
│              │                        │
│     ┌────────▼────────┐              │
│     │  ローカルLLMモデル  │              │
└─────────────────────────────────────┘
```

### 1.3 動作環境

- **OS**: Windows 10/11, macOS 11.0+, Linux (Ubuntu 20.04+等)
- **最小要件**:
  - RAM: 8GB以上（16GB推奨）
  - ストレージ: 10GB以上の空き容量
  - GPU: オプション（推奨、CPUでも動作可能）
- **ネットワーク**: インターネット接続（モデルダウンロード時のみ必要）

---

## 機能仕様

### 2.1 コア機能

#### 2.1.1 API作成機能

**機能ID**: F001  
**優先度**: 最高  
**概要**: ローカルLLM APIをワンクリックで作成する

**詳細仕様**:

1. **モデル選択**
   - [ ] 利用可能なLLMモデルリストを表示
   - [ ] モデル名、サイズ、説明を表示
   - [ ] モデル未インストール時はダウンロードオプション表示
   - [ ] 推奨モデルのハイライト表示

2. **設定**
   - [ ] API名の入力（オプション、デフォルト: "LocalAI API"）
   - [ ] ポート番号の指定（デフォルト: 8080）
   - [ ] 認証設定（APIキー有効化オプション）

3. **Ollama確認・インストール（自動）**
   - [ ] API作成前にOllamaのインストール状態を自動確認
   - [ ] 未インストールの場合は自動インストールを実行（F009参照）
   - [ ] インストール中の進捗表示

4. **作成・デプロイ**
   - [ ] 「APIを作成」ボタンのクリック
   - [ ] プログレスバーで進行状況を表示
   - [ ] 成功時にAPIエンドポイント情報を表示
   - [ ] エラー時はわかりやすいエラーメッセージ表示

**UIフロー**:
```
ホーム画面
  ↓
「新しいAPIを作成」ボタン
  ↓
Ollama検出（未インストール時は自動インストール）
  ↓
モデル選択画面
  ↓
設定画面（オプション）
  ↓
「作成」ボタン
  ↓
進行中表示
  ↓
成功画面（API情報表示）
```

#### 2.1.2 API利用機能

**機能ID**: F002  
**優先度**: 最高  
**概要**: 作成したAPIを安全に、簡単に利用する

**詳細仕様**:

1. **API一覧表示**
   - [ ] 作成済みAPIのリスト表示
   - [ ] API名、ステータス（実行中/停止）、エンドポイントURL表示
   - [ ] 起動/停止ボタン

2. **APIテスト**
   - [ ] テスト用のシンプルなチャットインターフェース
   - [ ] プロンプト入力欄
   - [ ] 送信ボタン
   - [ ] レスポンス表示エリア
   - [ ] タイムスタンプ、トークン数表示（オプション）

3. **API情報表示**
   - [ ] エンドポイントURL
   - [ ] APIキー（存在する場合、表示/非表示切り替え）
   - [ ] サンプルコード（curl、Python、JavaScript）
   - [ ] 「コピー」ボタン

**UIフロー**:
```
ホーム画面
  ↓
API一覧
  ↓
API選択
  ↓
API詳細画面（テスト/情報）
```

#### 2.1.3 API管理機能

**機能ID**: F003  
**優先度**: 高  
**概要**: 作成したAPIの管理

**詳細仕様**:

1. **起動/停止**
   - [ ] APIの起動ボタン
   - [ ] APIの停止ボタン
   - [ ] ステータス表示（実行中/停止中）

2. **設定変更**
   - [ ] ポート番号の変更
   - [ ] 認証設定の変更（APIキーの再生成等）

3. **削除**
   - [ ] 削除ボタン
   - [ ] 確認ダイアログ
   - [ ] 関連リソース（モデル等）の削除オプション

#### 2.1.4 モデル管理機能

**機能ID**: F004  
**優先度**: 高  
**概要**: LLMモデルの検索・ダウンロード・管理（LLMSTUDIO風の機能）

**詳細仕様**:

1. **モデル検索機能（強化）**
   - [ ] **リアルタイム検索バー**: モデル名、説明文からキーワード検索
   - [ ] **カテゴリフィルタ**: 
     - チャット/会話
     - コード生成
     - 翻訳
     - 要約
     - 質問応答
     - その他
   - [ ] **サイズフィルタ**: 
     - 小（1-3GB）
     - 中（3-7GB）
     - 大（7GB以上）
     - 全て
   - [ ] **用途フィルタ**:
     - 汎用
     - 専門用途（医療、法律、プログラミングなど）
   - [ ] **ソート機能**:
     - 人気順（ダウンロード数）
     - サイズ順（小→大、大→小）
     - 名前順（A-Z、Z-A）
     - 新着順
   - [ ] **検索結果の表示**: 検索に一致するモデルのリアルタイム表示

2. **モデルカタログ表示**
   - [ ] **モデルカード形式の表示**:
     - モデル名（大きく表示）
     - パラメータ数（例: 7B、13B、70B）
     - サイズ（GB単位、わかりやすく表示）
     - 簡易説明（2-3行）
     - カテゴリバッジ（チャット、コード生成など）
     - 推奨バッジ（🟢 推奨）
     - ダウンロードボタン
   - [ ] **詳細表示（モーダルまたは展開）**:
     - 完全な説明文
     - 使用例・サンプル
     - システム要件（推奨RAM、GPU要件など）
     - 作成者・ライセンス情報
     - 関連モデル（似たモデルの提案）
   - [ ] **人気モデルのハイライト**: 
     - 人気トップ5を上部に表示
     - 推奨モデルの強調表示
     - 初心者向けモデルの表示

3. **モデルダウンロード**
   - [ ] **ダウンロード開始**:
     - カードまたは詳細画面から「ダウンロード」ボタン
     - 確認ダイアログ（サイズ、必要容量の表示）
   - [ ] **ダウンロード進捗表示**:
     - プログレスバー（パーセンテージ表示）
     - ダウンロード速度（MB/s）
     - 残り時間（予測）
     - ダウンロード済みサイズ / 総サイズ
   - [ ] **中断・再開機能**:
     - ダウンロード中の「一時停止」ボタン
     - 「再開」ボタン（前回の続きから）
     - 「キャンセル」ボタン（削除オプション付き）
   - [ ] **バックグラウンドダウンロード**:
     - ダウンロード中でも他の機能を使用可能
     - 通知バーで進捗確認
     - 完了時の通知

4. **インストール済みモデル一覧**
   - [ ] **モデルカード表示**:
     - モデル名、パラメータ数、サイズ
     - インストール日時
     - 使用状況（最後に使用した日時）
     - 「削除」ボタン
   - [ ] **ソート・フィルタ**:
     - 名前順、サイズ順、インストール日時順
     - 使用頻度順
   - [ ] **クイックアクション**:
     - 「API作成に使用」ボタン（直接API作成画面へ）
     - 「削除」ボタン（確認ダイアログ付き）

5. **モデル情報の取得方法**
   - [ ] **Ollama公式モデルライブラリの統合**:
     - Ollamaが提供するモデルリストを使用
     - Ollama公式APIまたはモデルレジストリから情報取得
     - **重要**: Ollama公式サポートモデルのみを対象とする
       - 対応形式: GGUF形式のモデル（Ollama標準）
       - カスタムモデル（Hugging Faceから直接ダウンロードなど）は非対応（初期実装）
       - カスタムモデル対応は将来の機能拡張として検討（v2.0以降）
   - [ ] **ローカルキャッシュ**:
     - モデル情報をSQLiteにキャッシュ（初回取得後）
     - 定期的な更新（日次または週次）
   - [ ] **フォールバック**:
     - ネットワーク接続がない場合、キャッシュされた情報を表示
     - 基本的なモデル情報のみ表示（名前、サイズなど）

6. **非開発者向けUX改善**
   - [ ] **用語の置き換え**:
     - 「パラメータ」→「パラメータ数（大きいほど高性能）」
     - 「ダウンロード」→「モデルを取得」
   - [ ] **ガイダンス表示**:
     - 「初めての方へ」セクション（推奨モデルの紹介）
     - 「どのモデルを選べばいい？」ヘルプリンク
     - モデル選択のヒント表示
   - [ ] **視覚的な表示**:
     - モデルサイズのアイコン表示（📦 小、📦 中、📦 大）
     - カテゴリごとの色分け
     - 人気度のスター表示（⭐）

#### 2.1.5 Ollama自動インストール機能

**機能ID**: F009  
**優先度**: 最高（非開発者体験のため必須）  
**概要**: Ollamaが未インストールの場合、権限不要で自動セットアップする

**重要方針**: **非開発者でも管理者権限なしで使用できることが最優先**

**詳細仕様**:

1. **Ollama検出**
   - [ ] アプリ起動時にOllamaのインストール状態を自動検出
   - [ ] `ollama --version` コマンドで確認（システムパス上のOllama）
   - [ ] `http://localhost:11434/api/version` で確認（実行中のOllama）
   - [ ] FLMアプリディレクトリ内のポータブル版Ollamaを確認（優先）
   - [ ] 未検出の場合は自動セットアップを開始（確認ダイアログなし、自動実行）

2. **自動セットアップ（権限不要方式）**
   
   **優先方式1: ポータブル版Ollamaの使用（推奨・デフォルト）**
   - [ ] FLMアプリのインストールディレクトリ内にOllamaを配置
     - Windows: `%APPDATA%/FLM/ollama/` または `%LOCALAPPDATA%/FLM/ollama/`
     - macOS: `~/Library/Application Support/FLM/ollama/`
     - Linux: `~/.local/share/FLM/ollama/`
   - [ ] Ollamaのポータブル版バイナリを自動ダウンロード
     - Windows: `ollama-windows-amd64.exe` または `ollama.exe`
     - macOS: `ollama-darwin` (Intel/Apple Silicon自動判別)
     - Linux: `ollama-linux-amd64` または `ollama-linux-arm64`
   - [ ] ダウンロードURL: Ollama公式リリースページから最新版を取得
     - GitHub Releases APIを使用: `https://api.github.com/repos/ollama/ollama/releases/latest`
   - [ ] ダウンロード進捗表示（プログレスバー、パーセンテージ、速度）
   - [ ] ダウンロード完了後、自動的に実行権限を設定（Unix系OS）
   - [ ] 初回起動の自動実行（バックグラウンドプロセスとして起動）

   **優先方式2: アプリバンドル方式（推奨・優先方式）**
   - [ ] FLMインストーラーにOllamaバイナリを含める（デフォルト）
   - [ ] パッケージサイズ増加のトレードオフ（+50-100MB程度）を許容
   - [ ] **最大の利点**: インターネット接続不要、ダウンロード不要、即座に使用可能
   - [ ] 権限不要: バンドルされたバイナリはFLMと同じディレクトリに配置
   - [ ] バージョン固定: FLMがテスト済みのOllamaバージョンを同梱（互換性保証）
   - [ ] 自動更新: FLMアプリ更新時にOllamaも更新（オプション）

   **優先方式3: ポータブル版自動ダウンロード（フォールバック）**
   - [ ] バンドル版が利用できない場合のみ使用
   - [ ] 初回起動時にインターネット経由でダウンロード
   - [ ] 2回目以降はダウンロード不要

   **最後の手段: システムインストール（管理者権限必要）**
   - [ ] 上記全ての方式が失敗した場合のみ提示
   - [ ] 「システムにインストールするには管理者権限が必要です」と明示
   - [ ] 手動インストール手順のガイド表示（オプション）

3. **セットアップ後の自動起動**
   - [ ] FLMアプリ起動時に、ポータブル版Ollamaを自動起動
   - [ ] バックグラウンドプロセスとして管理（Tauriバックエンド）
   - [ ] Ollamaプロセスの状態監視（起動失敗時の自動リトライ）
   - [ ] FLMアプリ終了時にOllamaプロセスも終了（オプション、設定で変更可能）

4. **エラーハンドリング（非開発者向け）**
   - [ ] ダウンロード失敗時:
     - ネットワークエラーの場合: 「インターネット接続を確認してください」と表示
     - ディスク容量不足の場合: 「空き容量が必要です（最低1GB）」と表示
     - リトライボタンを大きく表示
   - [ ] 起動失敗時:
     - 「Ollamaの起動に失敗しました。もう一度お試しください」と表示
     - 自動リトライ（最大3回）
     - それでも失敗時: トラブルシューティングガイドを表示
   - [ ] エラーメッセージは専門用語を使わず、具体的なアクションを提示

**UIフロー**:
```
アプリ起動
  ↓
Ollama検出（システムインストール済み + ポータブル版）
  ↓
未検出の場合 → 自動的にポータブル版ダウンロード開始（通知のみ表示）
  ↓
ダウンロード進行（プログレスバー表示）
  ↓
ダウンロード完了 → 自動起動
  ↓
起動確認（3秒以内に応答確認）
  ↓
成功 → 通知「準備完了！」→ 通常のアプリ機能へ
失敗 → エラーメッセージ + リトライボタン表示
```

**実装方針**:
- ✅ **Tauriバックエンドで実装**: Rustでファイルダウンロード、解凍、実行権限設定
- ✅ **権限不要を最優先**: 管理者権限を必要としない実装を必須とする
- ✅ **自動実行**: ユーザーの確認を最小限に（初回のみ簡単な通知）
- ✅ **エラーハンドリング**: 失敗時の自動リトライ、わかりやすいエラーメッセージ
- ✅ **非同期処理**: UIブロックを避けるため、バックグラウンドで実行
- ✅ **セキュリティ**: ダウンロードしたバイナリのチェックサム検証（GitHub ReleasesのSHA256を使用）

**非開発者体験の要件**:
- ❌ **管理者権限は不要**（デフォルト方式）
- ❌ **ユーザー操作は最小限**（ダウンロード開始ボタンのみ、または完全自動）
- ❌ **専門用語は使用しない**（「ポータブル版」「バイナリ」などの用語はUIに表示しない）
- ✅ **進捗が常に見える**（プログレスバー、パーセンテージ、残り時間）
- ✅ **成功/失敗が明確**（大きな通知、色分け、アイコン）

### 2.2 セキュリティ機能

#### 2.2.1 認証機能

**機能ID**: F005  
**優先度**: 高  
**概要**: APIへのアクセス制御

**詳細仕様**:

1. **APIキー生成**
   - [ ] 自動生成された安全なAPIキー
   - [ ] 表示/非表示切り替え
   - [ ] 再生成機能

2. **認証方式**
   - [ ] Bearer Token認証
   - [ ] リクエストヘッダーでの認証
   - [ ] 認証失敗時の適切なエラーレスポンス

### 2.3 運用・監視機能

#### 2.3.1 ログ表示

**機能ID**: F006  
**優先度**: 中  
**概要**: APIの実行ログを表示

**詳細仕様**:

1. **ログ一覧**
   - [ ] リクエスト/レスポンスログ
   - [ ] タイムスタンプ、エンドポイント、ステータスコード
   - [ ] エラーログのハイライト表示

2. **ログフィルタリング**
   - [ ] 日時範囲フィルタ
   - [ ] ステータスコードフィルタ
   - [ ] エラー検索

#### 2.3.2 パフォーマンス監視

**機能ID**: F007  
**優先度**: 低（将来実装）  
**概要**: APIのパフォーマンス指標を表示

**詳細仕様**:
- [ ] リクエスト数
- [ ] 平均レスポンス時間
- [ ] エラー率
- [ ] トークン使用量

---

## 技術仕様

### 3.0 OSS使用方針

#### 3.0.1 基本方針

**FLMは、可能な限りOSS（オープンソースソフトウェア）を積極的に活用します。**

**方針**:
- [ ] **OSS優先**: プロプライエタリソフトウェアよりもOSSを優先して選択
- [ ] **ライセンス互換性**: 各OSSのライセンス要件を遵守
- [ ] **コミュニティ貢献**: 使用するOSSの改善に貢献（バグ報告、プルリクエスト等）
- [ ] **依存関係の明確化**: 使用する全てのOSSとそのバージョンを明記
- [ ] **セキュリティ**: 定期的な依存関係のセキュリティチェック

#### 3.0.1.1 OSS優先の判断基準

**OSS優先の原則**:

1. **選定プロセス**:
   - 機能要件が満たせるOSSが存在する場合、必ずOSSを選択
   - OSSが存在する場合でも、機能不足・パフォーマンス不足等の理由でプロプライエタリを選ぶ場合は、理由を文書化
   - 同等の機能・パフォーマンスであれば、必ずOSSを選択

2. **OSS選定の優先順位**:
   1. **既存OSSの直接利用**: 既存のOSSをそのまま利用（例: OllamaのAPIを直接利用）
   2. **OSSライブラリの統合**: OSSライブラリを組み合わせて実装
   3. **OSSフレームワークを使用**: OSSフレームワーク上で実装
   4. **最後の手段として独自実装**: OSSが存在しない場合のみ

3. **プロプライエタリ使用の例外条件**:
   - OSSで機能要件を満たせない場合のみ、例外としてプロプライエタリを検討
   - プロプライエタリ使用の場合は、以下を文書化:
     - 使用理由（OSSでは達成できない要件）
     - 代替OSSの調査結果
     - 将来のOSS移行計画（可能な場合）
     - コスト影響（有料の場合）

4. **OSS選定の具体例**:
   - ✅ **UIフレームワーク**: Tauri/Electron（OSS）を使用、プロプライエタリは使用しない
   - ✅ **LLM実行**: Ollama（OSS）を直接利用、独自実装はしない
   - ✅ **認証**: JWTライブラリ（OSS）を使用、プロプライエタリサービスは使用しない
   - ✅ **データベース**: SQLite（OSS）を使用、プロプライエタリDBは使用しない
   - ✅ **ホスティング**: GitHub Pages（OSSプロジェクトで無料）を優先

5. **OSS品質評価基準**:
   - **アクティビティ**: 過去6ヶ月以内にコミットがある
   - **コミュニティ**: スター数、フォーク数、Issue解決率
   - **ドキュメント**: 十分なドキュメントがある
   - **ライセンス**: MIT、Apache 2.0、BSD等の商用利用可能なライセンス

6. **OSS使用の記録**:
   - 全てのOSSライブラリを `LICENSES.md` に記載
   - 選定理由を文書化（特に複数の候補がある場合）
   - バージョンを固定（`package.json` / `requirements.txt` で管理）

#### 3.0.2 使用するOSSライブラリ・ツール

##### UIフレームワーク
- [ ] **Electron** (Apache 2.0) - クロスプラットフォームデスクトップアプリ
- [ ] **Tauri** (MIT/Apache 2.0) - 軽量なクロスプラットフォームフレームワーク
- [ ] **Flutter Desktop** (BSD-3-Clause) - Google製クロスプラットフォームフレームワーク

##### LLM実行エンジン
- [ ] **Ollama** (MIT) - ローカルLLM実行エンジン
- [ ] **llama.cpp** (MIT) - C++で実装されたLLM推論エンジン
- [ ] **vLLM** (Apache 2.0) - 高速なLLM推論サーバー

##### API・HTTP関連
- [ ] **Express.js** (MIT) - Node.js用Webアプリケーションフレームワーク
- [ ] **FastAPI** (MIT) - Python用モダンなWebフレームワーク
- [ ] **Axios** (MIT) - HTTPクライアント
- [ ] **Swagger/OpenAPI** (Apache 2.0) - API仕様定義

##### データベース・ストレージ
- [ ] **SQLite** (Public Domain) - 軽量データベース
- [ ] **LokiJS** (MIT) - インメモリデータベース

##### 認証・セキュリティ
- [ ] **jsonwebtoken** (MIT) - JWT認証
- [ ] **bcrypt** (MIT) - パスワードハッシュ化
- [ ] **crypto** (Node.js標準) - 暗号化機能

##### ユーティリティ
- [ ] **lodash** (MIT) - JavaScriptユーティリティライブラリ
- [ ] **date-fns** (MIT) - 日付操作ライブラリ
- [ ] **chalk** (MIT) - ターミナル文字列スタイリング

##### ログ・監視
- [ ] **Winston** (MIT) - Node.js用ロギングライブラリ
- [ ] **Pino** (MIT) - 高速なロガー

##### テスティング
- [ ] **Jest** (MIT) - JavaScriptテストフレームワーク
- [ ] **Pytest** (MIT) - Pythonテストフレームワーク
- [ ] **Playwright** (Apache 2.0) - E2Eテスト

##### ビルド・パッケージング
- [ ] **Webpack** (MIT) - モジュールバンドラー
- [ ] **Vite** (MIT) - 高速ビルドツール
- [ ] **electron-builder** (MIT) - Electronアプリのパッケージング
- [ ] **PyInstaller** (GPL) - Pythonアプリのパッケージング

##### CI/CD
- [ ] **GitHub Actions** - CI/CD（OSSプロジェクトで無料）
- [ ] **GitLab CI** - CI/CD（OSSで無制限）

##### ドキュメント
- [ ] **Docusaurus** (MIT) - ドキュメントサイト生成
- [ ] **MkDocs** (BSD-2-Clause) - Markdownベースのドキュメント生成

#### 3.0.3 ライセンス管理

**管理方法**:
- [ ] 使用する全てのOSSライブラリを `LICENSES.md` に記載
- [ ] `package.json` / `requirements.txt` で依存関係を明確化
- [ ] ライセンス互換性チェックツールを使用（例: `license-checker`）
- [ ] 定期的な依存関係の更新と脆弱性スキャン

**主要ライセンス**:
- **MIT**: 最も緩いライセンス、商用利用可能
- **Apache 2.0**: 特許保護付き、商用利用可能
- **BSD**: MITと類似、商用利用可能
- **GPL**: コピーレフト、派生作品もOSS化が必要

#### 3.0.4 OSSコミュニティへの貢献

**貢献方針**:
- [ ] 使用するOSSのバグ発見時は報告
- [ ] 機能改善のアイデアはIssueで提案
- [ ] 修正や機能追加はプルリクエストで提供
- [ ] ドキュメントの改善も貢献

**例**:
- Ollamaへの機能追加提案
- llama.cppのパフォーマンス改善
- Electronのクロスプラットフォーム問題の報告

#### 3.0.5 OSS活用による工数削減

**各フェーズでのOSS活用効果**:

| フェーズ | 使用OSS | 従来工数 | OSS活用後 | 削減効果 |
|---------|---------|---------|-----------|---------|
| LLM実行エンジン | Ollama | 4-6週間 | **2-3週間** | ⭐⭐⭐⭐⭐ |
| UIフレームワーク | Tauri | 6-8週間 | **4-5週間** | ⭐⭐⭐⭐ |
| インストーラー作成 | Tauri内蔵 | 2-3週間 | **1週間** | ⭐⭐⭐⭐⭐ |
| APIサーバー実装 | Ollama API直接利用 | 4-5週間 | **1-2週間** | ⭐⭐⭐⭐⭐ |
| 認証・セキュリティ | JWTライブラリ等 | 3-4週間 | **2-3週間** | ⭐⭐⭐ |

**合計削減効果**: 約2.5-4ヶ月（約30-40%の開発工数削減）

**削減の根拠**:
- Ollamaがモデル管理・API実行・OpenAI互換APIを既に提供
- Tauriがインストーラー自動生成機能を内蔵
- 既存OSSのAPIを直接利用することで、実装工数を大幅削減

### 3.1 アーキテクチャ

#### 3.1.0 OSS統合アーキテクチャ

**全体アーキテクチャ図**:
```
┌─────────────────────────────────────────┐
│   FLM (Tauri - OSS)             │
│   ┌─────────────────────────────────┐ │
│   │  React/Vue Frontend (OSS)        │ │
│   │  - UIコンポーネント                │ │
│   │  - 状態管理                        │ │
│   └──────────────┬──────────────────┘ │
│                  │ IPC                   │
│   ┌──────────────▼──────────────────┐ │
│   │  Tauri Backend (Rust - OSS)     │ │
│   │  - プロセス管理                  │ │
│   │  - APIキー管理                   │ │
│   │  - Ollamaプロセス制御             │ │
│   └──────────────┬──────────────────┘ │
└──────────────────┼──────────────────────┘
                   │ HTTP Proxy (認証レイヤー)
┌──────────────────▼──────────────────────┐
│  認証プロキシ (express-http-proxy - OSS) │
│  - APIキー検証                          │
│  - リクエストログ                        │
└──────────────────┬──────────────────────┘
                   │ HTTP
┌──────────────────▼──────────────────────┐
│   Ollama REST API (OSS)                 │
│   - モデル管理 (GET /api/tags)           │
│   - モデルダウンロード (POST /api/pull)   │
│   - LLM推論実行 (POST /api/chat)         │
│   - OpenAI互換API提供                    │
└─────────────────────────────────────────┘
                   │
┌──────────────────▼──────────────────────┐
│   ローカルLLMモデル (Ollama管理)           │
│   - Windows: %USERPROFILE%\.ollama\models\│
│   - macOS: ~/.ollama/models/             │
│   - Linux: ~/.ollama/models/            │
└─────────────────────────────────────────┘
```

**実装方針**:
- ✅ **OSSを直接利用**: 既存のOSS（Ollama、Tauri等）の機能を再発明しない
- ✅ **最小限のラッパー**: 認証・アクセス制御レイヤーのみ追加実装
- ✅ **API直接利用**: OllamaのREST APIを直接呼び出し、プロキシ経由で認証

#### 3.1.1 UIフレームワーク

**推奨選定**: **Tauri**（初期実装として優先）

**選定理由**:
- ✅ **インストーラー自動生成**: 約1-2週間の工数削減（electron-builder等が不要）
- ✅ **軽量**: Electronの約1/10サイズ、起動速度向上
- ✅ **セキュリティ**: CSP、権限管理が内蔵、デフォルトでセキュア
- ✅ **クロスプラットフォーム**: Windows、macOS、Linuxの統一ビルド
- ✅ **開発効率**: JavaScript/TypeScript対応、RustバックエンドでOllama統合が容易

**比較表**:
| 項目 | Tauri | Electron | Flutter Desktop |
|------|-------|----------|----------------|
| パッケージサイズ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ |
| 起動速度 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ |
| 開発容易性 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| インストーラー機能 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ |
| セキュリティ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ |
| コミュニティ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |

**選定基準**:
- ✅ OSSであること（必須）
- ✅ クロスプラットフォーム対応
- ✅ ネイティブに近いパフォーマンス
- ✅ セキュリティ
- ✅ パッケージサイズ
- ✅ アクティブなコミュニティとメンテナンス

**最終選定**: MVP開発開始前に1週間の技術検証（プロトタイプ作成）で確定

**代替案**:
- **Electron** (MIT) - Tauriで問題が発生した場合の代替案
- **Flutter Desktop** (BSD-3-Clause) - UIパフォーマンス重視の場合

#### 3.1.2 LLM実行エンジン

**選定**: **Ollama**（初期実装として必須）

**選定OSS**:
- **Ollama** (MIT) - **推奨・優先・必須**: ユーザーフレンドリーなAPI、REST API提供、OpenAI互換
- **llama.cpp** (MIT) - Ollama内部で使用、直接統合は不要（v2.0以降で代替実装として検討可能）
- **vLLM** (Apache 2.0) - 将来の拡張として検討（高性能が必要な場合）

**改善策: 依存性リスクの軽減**:
- ✅ **バージョン固定**: FLMは特定バージョンのOllamaに固定（互換性保証）
- ✅ **バンドル方式優先**: OllamaをFLMアプリに同梱（バージョン管理可能、外部依存軽減）
- ✅ **互換性レイヤー**: Ollama APIの変更に備えた抽象化レイヤー（将来実装）
- ✅ **フォールバック対応**: 複数のOllamaバージョンに対応（互換性モード）

**統合方法**:
- ✅ **Ollamaのバンドル方式（推奨・最優先）**: FLMアプリにOllamaバイナリを同梱
  - **メリット**:
    - ✅ インターネット接続不要（初回起動時もダウンロード不要）
    - ✅ バージョン固定による互換性保証（FLMがテスト済みのバージョンを使用）
    - ✅ 権限不要（FLMと同じディレクトリに配置）
    - ✅ 即座に使用可能（セットアップ時間ゼロ）
    - ✅ 外部依存の最小化（Ollamaの開発状況に左右されにくい）
  - **配置場所**: FLMアプリのインストールディレクトリ内（例: `FLMインストール先/ollama/`）
  - **パッケージサイズ**: +50-100MB（許容範囲）

- ✅ **フォールバック: ポータブル版自動ダウンロード**: バンドル版が利用できない場合のみ
  - Windows: `%LOCALAPPDATA%/FLM/ollama/`
  - macOS: `~/Library/Application Support/FLM/ollama/`
  - Linux: `~/.local/share/FLM/ollama/`
  - 初回起動時にインターネット経由でダウンロード

- ✅ **最後の手段: システムインストール済みOllamaの検出・使用**
  - システムパス上のOllamaを優先検出
  - 互換性チェックを実施（バージョン確認）
  - 非互換の場合はバンドル版を使用

**重要**: 非開発者向けツールのため、事前インストール不要、管理者権限不要を最優先
- ✅ **OllamaのREST APIを直接利用**: モデル管理、LLM推論、OpenAI互換APIはOllamaが提供
- ✅ **プロセス管理のみ実装**: Ollamaの起動・停止・状態監視をTauriバックエンドで管理

**Ollamaの主要エンドポイント（直接利用）**:
- `GET /api/tags` - インストール済みモデル一覧取得
- `POST /api/pull` - モデルダウンロード（進捗ストリーミング対応）
- `POST /api/chat` - チャット補完（OpenAI互換）
- `POST /api/generate` - テキスト生成
- `POST /api/delete` - モデル削除
- `GET /api/version` - Ollamaバージョン確認

**削減効果**:
- モデル管理機能: 約2週間の工数削減（Ollama API使用）
- API実装: 約3週間の工数削減（Ollamaが既に提供）
- OpenAI互換性: 約1週間の工数削減（Ollamaが既に対応）

#### 3.1.3 APIサーバー（認証プロキシ）

**実装方針**: **OllamaのREST APIを直接利用し、認証・アクセス制御レイヤーのみ追加**

**アーキテクチャ**:
```
クライアント
  ↓ HTTP (Authorization: Bearer <API_KEY>)
認証プロキシ (express-http-proxy - OSS)
  ↓ HTTP (認証済み)
Ollama REST API (OSS)
  ↓
LLM推論実行
```

**選定OSS**:
- **Node.js + Express.js** (MIT) - 認証プロキシ実装用
- **express-http-proxy** (MIT) - HTTPプロキシライブラリ
- **jsonwebtoken** (MIT) - APIキー検証

**実装詳細**:
- ✅ **Ollama APIの直接利用**: `/api/chat`、`/api/generate`等はOllamaが提供
- ✅ **認証レイヤーのみ追加**: APIキー検証後、Ollama APIへ転送
- ✅ **軽量プロキシ**: express-http-proxy等のOSSライブラリで実装
- ✅ **レスポンス転送**: Ollamaからのレスポンスをそのまま返却

**エンドポイント構成**:
```
FLM提供エンドポイント:
  POST /v1/chat/completions  → Ollama /api/chat (認証付き)
  GET /v1/models            → Ollama /api/tags (認証付き)
  
Ollama直接エンドポイント（プロキシ経由）:
  POST /api/pull            → Ollama /api/pull (認証付き)
  POST /api/delete          → Ollama /api/delete (認証付き)
```

**ポート構成**:
- **Ollama**: デフォルト `http://localhost:11434`（固定、Ollama標準）
- **認証プロキシ**: デフォルト `http://localhost:8080`（変更可能）

**OSS仕様準拠**:
- OpenAI API仕様（Ollamaが既に準拠）
- OpenAPI 3.0仕様（Ollama API仕様を参照）

**削減効果**: APIサーバー実装工数を約4-5週間から1-2週間へ削減（約70%削減）

### 3.2 データ管理

#### 3.2.1 データストレージ

**保存データ**:
- API設定（名前、ポート、認証設定等）
- モデル情報（インストール済みモデルのパス）
- ユーザー設定（テーマ、言語等）

**保存場所**:
- Windows: `%APPDATA%/FLM/`
- macOS: `~/Library/Application Support/FLM/`
- Linux: `~/.config/FLM/`

**形式**: 
- **SQLite** (Public Domain) - 推奨。OSSデータベース
- **JSONファイル** - シンプルな設定データ用

### 3.3 依存関係管理

#### 3.3.1 LLMモデル

**管理方針**: **Ollamaのモデル管理機能を直接利用（再発明しない）+ モデルカタログ機能の追加**

**統合方法**:
- ✅ **Ollama APIを使用**: モデルのダウンロード、一覧、削除は全てOllama API経由
- ✅ **Ollamaの保存場所を使用**: 別途モデル保存場所を作成しない
- ✅ **進捗管理**: OllamaのストリーミングAPI (`POST /api/pull`) を利用
- ✅ **モデルカタログ**: Ollama公式モデルライブラリから情報を取得（LLMSTUDIO風の機能追加）

**Ollama API使用例**:
```javascript
// インストール済みモデル一覧取得
GET http://localhost:11434/api/tags

// モデルダウンロード（進捗ストリーミング）
POST http://localhost:11434/api/pull
{
  "name": "llama2",
  "stream": true  // 進捗情報をストリーミング
}

// モデル削除
POST http://localhost:11434/api/delete
{
  "name": "llama2"
}
```

**モデルカタログ情報の取得方法**:
- ✅ **Ollama公式モデルライブラリ**: 
  - Ollamaが提供する利用可能モデルリストから情報を取得
  - モデル名、サイズ、説明、カテゴリなどの情報を含む
- ✅ **ローカルキャッシュ（SQLite）**:
  - モデル情報を`models_catalog`テーブルにキャッシュ
  - 初回起動時または定期的に（日次/週次）更新
  - オフライン時も基本的な情報を表示可能
- ✅ **検索・フィルタリング**:
  - SQLiteでローカル検索（高速）
  - キーワード、カテゴリ、サイズでのフィルタリング
  - ソート機能（人気順、サイズ順など）

**Ollamaのモデル対応状況と制限**:
- ✅ **公式サポートモデル**: Ollamaが公式にサポートしているモデル
  - Llama 2、Llama 3、Mistral、Vicuna、LLaVA、CodeLlama、Phi-3など
  - これらは`ollama pull <model名>`で直接ダウンロード・インストール可能
  - FLMではこれらのモデルを簡単に検索・ダウンロード可能
- ⚠️ **モデル形式の制限**: 
  - Ollamaは主に**GGUF形式**のモデルをサポート
  - 他の形式（PyTorch、TensorFlow、ONNXなど）は直接サポートされていない
  - 非対応形式のモデルは、GGUF形式に変換する必要がある（技術的知識が必要）
- ⚠️ **カスタムモデルの制限**:
  - Hugging Faceなどから独自にダウンロードしたモデルは、Ollama形式に変換が必要
  - `Modelfile`という設定ファイルを作成し、`ollama create`コマンドで登録する必要がある
  - **非開発者には難しい**（FLMでは対応困難）
- ✅ **推奨アプローチ（段階的）**: 
  - **v1.0（初期実装）**: Ollama公式モデルのみをサポート
    - 非開発者でも簡単に使えることを最優先
    - カスタムモデルは技術的障壁が高いため、初期実装では除外
  - **v2.0以降（将来拡張）**: LLMSTUDIO風の機能を追加予定
    - Hugging Face統合によるカスタムモデル対応
    - Modelfile作成支援（GUIで作成可能に）
    - モデル変換機能（可能な範囲で自動化）
    - 複数モデル形式への対応拡張
    - **注意**: カスタムモデル対応は技術的知識が必要なため、上級者向け機能として提供

**データ構造例（SQLite）**:
```sql
CREATE TABLE models_catalog (
  id TEXT PRIMARY KEY,
  name TEXT NOT NULL,
  description TEXT,
  category TEXT,  -- 'chat', 'code', 'translation', etc.
  size_gb REAL,
  parameters TEXT,  -- '7B', '13B', etc.
  popularity INTEGER DEFAULT 0,
  recommended BOOLEAN DEFAULT FALSE,
  license TEXT,
  author TEXT,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Ollamaのデフォルトモデル保存場所**:
- **Windows**: `%USERPROFILE%\.ollama\models\`
- **macOS**: `~/.ollama/models/`
- **Linux**: `~/.ollama/models/`

**FLMが管理するデータ**:
- ✅ API設定（名前、ポート、認証設定） - SQLiteに保存
- ✅ APIキー（暗号化保存） - SQLiteに保存
- ✅ ユーザー設定（テーマ、言語等） - SQLiteに保存
- ✅ Ollamaプロセスの状態管理 - メモリ上で管理

**モデル管理機能の実装**:
- モデル一覧: Ollama API (`GET /api/tags`) を呼び出し
- モデルダウンロード: Ollama API (`POST /api/pull`) を呼び出し、進捗をUIに表示
- モデル削除: Ollama API (`POST /api/delete`) を呼び出し

**削減効果**: モデル管理機能の実装工数を約2週間削減（Ollama API直接利用）

---

## UI/UX仕様

### 4.1 デザイン原則

1. **シンプル**: 情報を最小限に、一度に表示する要素を制限
2. **直感的**: 専門用語を避け、わかりやすい言葉を使用
3. **一貫性**: 全OSで同じUI/UX、統一されたデザイン言語
4. **ガイダンス**: 各ステップで明確な説明と次のアクションを提示

### 4.2 画面構成

#### 4.2.1 ホーム画面

**レイアウト**:
```
┌─────────────────────────────────┐
│  FLM              [設定]  │
├─────────────────────────────────┤
│                                   │
│   「新しいAPIを作成」ボタン          │
│                                   │
│  ┌────────────────────────────┐  │
│  │  作成済みAPI一覧              │  │
│  │  - API 1 [実行中] [テスト]   │  │
│  │  - API 2 [停止中] [テスト]   │  │
│  └────────────────────────────┘  │
│                                   │
└─────────────────────────────────┘
```

#### 4.2.2 モデル選択画面

**要素**:
- モデルカード（画像・アイコン、名前、サイズ、説明）
- 「推奨」バッジ
- フィルタリングUI（サイズ、用途）
- 「次へ」ボタン

#### 4.2.3 API作成画面（設定）

**要素**:
- API名入力欄
- ポート番号入力欄
- 認証設定（チェックボックス）
- 「作成」ボタン
- 「戻る」ボタン

#### 4.2.4 API詳細画面

**タブ構成**:
1. **テスト**: チャットインターフェース
2. **情報**: エンドポイントURL、APIキー、サンプルコード
3. **ログ**: 実行ログ

### 4.3 用語集（非開発者向け）

| 技術用語 | 表示用語 | 説明 |
|---------|---------|------|
| デプロイ | 公開 | APIを使える状態にする |
| エンドポイント | 接続先 | APIのURL |
| APIキー | アクセスキー | 安全にAPIを使うための鍵 |
| モデル | AI | 質問に答えてくれるAI |

### 4.4 エラー処理

**エラーメッセージの要件**:
- 専門用語を使わない
- 具体的な解決方法を提示
- 視覚的に目立つ（赤色、アイコン）
- 「ヘルプ」リンクを提供

**例**:
```
❌ エラー: APIの作成に失敗しました

ポート8080が既に使用されています。
別のポート番号（例: 8081）を試してください。

[ポート番号を変更] [ヘルプを見る]
```

---

## API仕様

### 5.1 Ollama API統合（基盤）

**実装方針**: Ollamaが提供するREST APIを直接利用

**Ollamaの主要エンドポイント**（FLMから直接利用）:
- `POST /api/chat` - チャット補完（OpenAI互換形式）
- `POST /api/generate` - テキスト生成
- `GET /api/tags` - インストール済みモデル一覧取得
- `POST /api/pull` - モデルダウンロード（進捗ストリーミング対応）
- `POST /api/delete` - モデル削除
- `GET /api/version` - Ollamaバージョン確認

**Ollama API仕様**:
- 公式ドキュメント: https://github.com/ollama/ollama/blob/main/docs/api.md
- OpenAI互換形式をサポート（`/api/chat`エンドポイント）
- ストリーミングレスポンス対応

### 5.2 FLM拡張API（認証レイヤー）

**アーキテクチャ**: Ollama APIの前に認証プロキシを配置

**エンドポイント**: `POST /v1/chat/completions`（Ollama互換、認証付き）

**認証フロー**:
1. クライアントは `Authorization: Bearer <API_KEY>` ヘッダーを送信
2. 認証プロキシでAPIキーを検証
3. 検証成功後、リクエストをOllama API (`http://localhost:11434/api/chat`) へ転送
4. Ollamaからのレスポンスをそのままクライアントへ返却

**実装**: 
- express-http-proxy (MIT) - HTTPプロキシライブラリ
- jsonwebtoken (MIT) - APIキー検証

#### 5.2.1 チャット補完（認証付き）

**エンドポイント**: `POST /v1/chat/completions`

**リクエスト例**:
```json
{
  "model": "local-llm",
  "messages": [
    {"role": "user", "content": "Hello, how are you?"}
  ],
  "temperature": 0.7,
  "max_tokens": 100
}
```

**レスポンス例**:
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "local-llm",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "I'm doing well, thank you!"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```

#### 5.2.2 モデル一覧（認証付き）

**エンドポイント**: `GET /v1/models`

**レスポンス例**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "local-llm",
      "object": "model",
      "created": 1677610602,
      "owned_by": "local"
    }
  ]
}
```

### 5.2 認証

**ヘッダー**:
```
Authorization: Bearer <API_KEY>
```

**エラーレスポンス**:
```json
{
  "error": {
    "message": "Invalid API key",
    "type": "authentication_error",
    "code": "invalid_api_key"
  }
}
```

---

## セキュリティ仕様

### 6.1 認証・認可

1. **APIキー**
   - 長さ: 32文字以上
   - 形式: ランダム文字列（英数字、記号）
   - 保存: 暗号化して保存

2. **APIキー管理**
   - 自動生成
   - 再生成機能
   - 表示/非表示切り替え

### 6.2 通信セキュリティ

1. **ローカルホストのみ**
   - デフォルト: `localhost:8080`
   - 外部からのアクセスは基本的に許可しない

2. **HTTPS対応（将来的に）**
   - 自己署名証明書の生成
   - ローカル環境での安全な通信

### 6.3 データ保護

1. **モデルファイル**
   - 暗号化保存（オプション）
   - アクセス権限管理

2. **ログ**
   - 個人情報の自動マスキング
   - ログ保持期間の設定

---

## パフォーマンス要件

### 7.1 応答時間

- **API作成**: 30秒以内（モデルインストール済みの場合）
- **API起動**: 10秒以内
- **APIレスポンス**: モデル依存（目標: 1-5秒）

### 7.1.1 初回インストールからAPI発行までの所要時間見積もり

**最良ケース（最短時間）**: **約1-2分**

**内訳**:
1. **FLMアプリインストール**: 30秒（インストーラー実行、完了確認）
2. **アプリ起動**: 5秒（初回起動時の初期化）
3. **Ollamaセットアップ**: 0秒（バンドル版を使用、既に同梱済み）
4. **モデル選択**: 10秒（ユーザー操作: 既にインストール済みモデルを選択）
5. **API作成実行**: 30秒（仕様書の目標値）
   - APIキー生成: 1秒
   - Ollama起動確認: 5秒
   - 認証プロキシ起動: 3秒
   - 設定保存: 1秒
   - 成功画面表示: 20秒（ユーザー確認含む）
6. **API情報確認**: 5秒（エンドポイントURL、APIキーの確認）

**合計**: 約1分20秒〜1分50秒

**前提条件**:
- Ollamaがバンドル版（FLMアプリに同梱）を使用
- モデルが既にインストール済み（または非常に小さいモデル（1-2GB）で高速ダウンロード）
- ユーザーが迷わず操作できる（UIが直感的）
- ネットワークが高速（モデルダウンロード必要な場合）

---

**通常ケース**: **約3-5分**

**追加時間**:
- モデルダウンロード: 1-3分（モデルサイズとネットワーク速度に依存）
  - 小サイズモデル（3GB）: 1分（高速回線）〜 3分（通常回線）
  - 中サイズモデル（7GB）: 2-4分（高速回線）〜 5-7分（通常回線）

**最悪ケース**: **約10-15分**

**追加時間**:
- Ollamaダウンロード（バンドル版が利用不可の場合）: 1-2分
- 大サイズモデル（13GB以上）のダウンロード: 5-10分（ネットワーク速度に依存）
- ユーザーが操作に迷う場合: +2-5分

---

**目標達成の評価**:
- **目標**: 非開発者が5分以内でローカルLLM APIを作成し、利用できる
- **最良ケース**: ✅ 達成（約1-2分）
- **通常ケース**: ✅ 達成（約3-5分、モデルダウンロード含む）
- **最悪ケース**: ❌ やや超過（約10-15分、大サイズモデルダウンロード時）

**最良ケースを実現するための要件**:
1. ✅ Ollamaバンドル版を使用（セットアップ時間ゼロ）
2. ✅ 推奨モデルの事前ダウンロード（または小サイズモデルを推奨）
3. ✅ 直感的なUI（ユーザーが迷わない設計）
4. ✅ 高速なネットワーク環境（モデルダウンロード時）

### 7.2 リソース使用量

- **メモリ**: 使用モデルサイズ + 2GB（アプリ自体）
- **CPU**: アイドル時 < 5%
- **ディスク**: モデルサイズ + 500MB（アプリ）

### 7.3 同時リクエスト

- 初期実装: 1リクエスト/秒
- 将来: 複数リクエストのキュー管理

---

## 非機能要件

### 8.1 可用性

- アプリのクラッシュ率: < 1%
- エラー回復: 自動リトライ機能

### 8.2 保守性

- コードコメント
- ドキュメント整備
- ログ機能

### 8.3 拡張性

**将来の拡張計画**:
- ✅ **プラグインアーキテクチャ**: サードパーティによる機能拡張のための基盤（v2.0以降）
- ✅ **カスタムモデル対応**: LLMSTUDIO風の機能を追加（v2.0以降）
  - Hugging Face統合
  - モデル変換機能
  - Modelfile作成支援
  - 複数形式対応
- ✅ **複数LLM実行エンジン対応**: 
  - 現状: Ollamaのみ
  - v1.3: LM Studio対応（抽象化レイヤー実装）
  - v2.0: llama.cpp、vLLM、text-generation-webui対応
  - v2.0以降: カスタムエンドポイント対応
  - **実装方針**: Strategy/Adapterパターンで実行エンジンを抽象化し、統一インターフェースを提供
- ✅ **クラウドデプロイ**: AWS、Azure、GCPへのデプロイ対応（v2.0以降）

**拡張性の設計方針**:
- ✅ **抽象化レイヤー**: LLM実行エンジンを抽象化し、将来的な差し替えを容易に
- ✅ **モジュラー設計**: 機能をモジュール化し、追加・削除を容易に
- ✅ **設定ファイルの拡張性**: JSON Schema等で設定項目を拡張可能に
- ✅ **APIの一貫性**: 拡張機能でも同じAPIインターフェースを維持

### 8.4 アクセシビリティ

- キーボードナビゲーション対応
- スクリーンリーダー対応（基本的な対応）

---

## 開発・テスト方針

### 9.1 開発環境

**OSSツールを積極的に使用**:

- **バージョン管理**: Git (OSS)
- **CI/CD**: 
  - GitHub Actions (OSSプロジェクトで無料)
  - GitLab CI (OSSで無制限)
- **テストフレームワーク**: 
  - Jest (MIT) - JavaScript
  - Pytest (MIT) - Python
  - Playwright (Apache 2.0) - E2Eテスト
- **コードエディタ**: 
  - VS Code (MIT) - 推奨
  - Vim/Neovim (OSS) - オプション
- **パッケージマネージャー**:
  - npm/yarn (OSS)
  - pip (OSS)

### 9.2 テスト計画

#### 9.2.1 単体テスト

- 各機能モジュールのテスト
- カバレッジ目標: 80%以上

#### 9.2.2 統合テスト

- LLM実行エンジンとの統合
- APIエンドポイントのテスト

#### 9.2.3 UIテスト

- 主要なユーザーフローのテスト
- クロスプラットフォームテスト

#### 9.2.4 ユーザーテスト

- 非開発者10-20人でのベータテスト
- 「5分でAPI作成」の目標検証

### 9.3 リリース計画

#### MVP（最小実用製品） - 3-4ヶ月

**目標**: 非開発者が5分以内でローカルLLM APIを作成し、利用できる

**必須機能**:
- ✅ API作成機能（F001）- Ollama統合、モデル選択、基本設定
- ✅ API利用機能（F002）- 基本的なテスト機能、API情報表示
- ✅ API管理機能（F003）- 起動/停止、基本的な設定変更
- ✅ モデル管理機能（F004）- Ollama API経由でのダウンロード・一覧表示
- ✅ 認証機能（F005）- 基本的なAPIキー生成・検証
- ✅ Ollama自動インストール機能（F009）- 非開発者体験のため必須

**技術スタック（MVP）**:
- UI: Tauri（推奨）またはElectron
- LLM実行: Ollama（必須、自動インストール機能付き）
- 認証: 簡易プロキシ（express-http-proxy等）
- データ保存: SQLite
- OS対応: Windowsのみ（初期実装）

**除外機能（v1.0以降）**:
- ❌ ログ表示（F006）- v1.1で実装
- ❌ パフォーマンス監視（F007）- v1.1で実装
- ❌ 高度な認証（OAuth等）- v2.0で実装
- ❌ macOS/Linux対応 - v1.0で実装

**OSS活用による削減効果**:
- LLM実行エンジン: Ollama使用で約2-3週間削減
- API実装: Ollama API直接利用で約3週間削減
- インストーラー: Tauri内蔵機能で約1-2週間削減

#### v1.0 - MVP後2-3ヶ月

**追加機能**:
- ✅ 全コア機能実装
- ✅ クロスプラットフォーム対応（macOS、Linux）
- ✅ 運用・監視機能（F006、F007）
- ✅ 高度なエラーハンドリング
- ✅ 使い方ガイド自動生成

#### v1.1 - v1.0後1-2ヶ月

**追加機能**:
- ✅ ログ表示・フィルタリング強化
- ✅ パフォーマンス監視ダッシュボード
- ✅ 複数API同時実行対応

#### v2.0 - v1.1後（将来拡張）

**追加機能**:
- クラウドデプロイ対応
- 高度な認証（OAuth2等）
- プラグインアーキテクチャ
- **LLMSTUDIO風の高度なモデル管理機能**:
  - ✅ **カスタムモデル対応**: Hugging Faceなどからの直接ダウンロード・変換
  - ✅ **Modelfile作成支援**: カスタムモデルのModelfileをGUIで作成・編集
  - ✅ **モデル変換機能**: GGUF形式への自動変換（可能な範囲で）
  - ✅ **複数モデル形式対応**: PyTorch、ONNX等の形式への対応拡張（検討）
  - ✅ **モデル共有機能**: 作成したカスタムモデルをコミュニティと共有
  - ✅ **高度なモデル検索**: Hugging Face APIとの統合による検索拡張

---

### 9.4 公式Webサイト仕様

**機能ID**: F008  
**優先度**: 高  
**概要**: FLMの公式Webサイト。各OS向けインストーラーの直接ダウンロードと情報提供

**詳細仕様書**: `DOCKS/公式サイト仕様書.md` を参照してください。

#### 9.4.1 基本方針

**重要**: 公式Webサイトは**静的サイト**として実装します。フレームワークやビルドツールは一切使用しません。

- **技術**: HTML、CSS、JavaScriptのみ（静的サイト、フレームワーク不使用）
- **フレームワーク不使用**: React、Vue、Angular等のJavaScriptフレームワークは使用しない
- **ビルドツール不使用**: Webpack、Vite等のビルドツールは使用しない
- **バックエンド不要**: サーバーサイド言語（PHP、Python、Node.js等）は使用しない
- **データベース不要**: 静的コンテンツのみ
- **ホスティング**: GitHub Pages（推奨、無料、OSSプロジェクト向け）

#### 9.4.2 必須ページ

1. **ホームページ（index.html）**: プロダクト紹介、ダウンロードセクション
2. **ダウンロードページ（download.html）**: 各OS向けインストーラー、システム要件
3. **機能紹介ページ（features.html）**: 主要機能の説明
4. **ドキュメント**: 使い方ガイド、FAQ、トラブルシューティング
5. **サポート**: GitHub Issuesリンク、コミュニティリンク

#### 9.4.3 必須機能

- ✅ OS自動検出機能（JavaScript）
- ✅ レスポンシブデザイン（モバイル対応）
- ✅ HTTPS必須
- ✅ アクセシビリティ対応（WCAG 2.1 AA準拠）

**詳細仕様**: `DOCKS/公式サイト仕様書.md` を参照してください。

---

## 用語集

### 10.1 技術用語

- **LLM**: Large Language Model（大規模言語モデル）
- **API**: Application Programming Interface（アプリケーション間の通信規約）
- **エンドポイント**: APIのURLアドレス
- **デプロイ**: ソフトウェアを実行可能な状態にすること
- **トークン**: LLMが処理する最小単位のテキスト

### 10.2 非技術用語（UI表示用）

- **API**: AIとの会話の接続先
- **エンドポイント**: 接続先のURL
- **デプロイ**: 公開
- **モデル**: AI

---

## 付録

### A. 参考資料

- コンセプトドキュメント: `CONCEPT.md`
- UI/UXデザイン: （将来追加）

### B. 変更履歴

| バージョン | 日付 | 変更内容 | 作成者 |
|-----------|------|---------|--------|
| 1.0.0 | 2024年 | 初版作成 | - |
| 1.1.0 | 2024年 | OSS統合方法の詳細化、Tauri推奨選定、Ollama API直接利用方針、MVP範囲の明確化を追記 | - |
| 1.2.0 | 2024年 | 公式Webサイト仕様を追加（HTML/CSS/JavaScriptのみ、静的サイト） | - |
| 1.2.1 | 2024年 | OSS優先の判断基準を明確化（選定プロセス、優先順位、例外条件、品質評価基準を追加） | - |
| 1.2.2 | 2024年 | Ollama自動インストール機能（F009）を追加。非開発者体験のため事前インストール不要に変更。統合方法を更新（自動インストール必須化）。API作成機能にOllama検出・インストールステップを追加 | - |
| 1.2.3 | 2024年 | 公式Webサイト仕様を整理。詳細仕様はDOCKS/公式サイト仕様書.mdを参照 | - |
| 1.2.4 | 2024年 | Ollama自動セットアップ機能（F009）を大幅改善。管理者権限不要のポータブル版方式をデフォルトに変更。非開発者でも簡単に使用できるよう、権限不要・自動実行・わかりやすいエラーメッセージを実装方針として明確化。システムインストールはフォールバック方式に変更。 | - |
| 1.2.5 | 2024年 | Ollama依存リスクの軽減策を追加。アプリバンドル方式を最優先方式に変更（インターネット不要・即座使用可能・バージョン固定による互換性保証）。依存性リスク軽減のため、バージョン固定・互換性レイヤー・フォールバック対応を実装方針に追加。 | - |
| 1.2.6 | 2024年 | モデル管理機能（F004）を大幅強化。LLMSTUDIO風のモデル検索・ダウンロード機能を追加。リアルタイム検索、カテゴリフィルタ、サイズフィルタ、ソート機能、モデルカタログ表示、詳細情報表示、バックグラウンドダウンロードなどの機能を追加。モデルカタログ情報をSQLiteにキャッシュする方式を追加。 | - |
| 1.2.7 | 2024年 | 将来の拡張計画を明確化。v2.0以降でLLMSTUDIO風の高度なモデル管理機能（カスタムモデル対応、Hugging Face統合、Modelfile作成支援、モデル変換機能）を追加予定であることを明記。拡張性の設計方針（抽象化レイヤー、モジュラー設計）を追加。 | - |

---

**文書終了**

