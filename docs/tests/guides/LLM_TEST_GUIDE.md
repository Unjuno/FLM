# LLM実行テストガイド

このドキュメントでは、アプリケーション内のLLM（API）の実行テスト方法を説明します。

## テスト方法

### 方法1: アプリ内のテストページを使用（推奨）

1. **アプリケーションを起動**
   ```bash
   npm run tauri:dev
   ```

2. **APIを作成・起動**
   - アプリ内で「API一覧」ページに移動
   - 「新しいAPIを作成」をクリック
   - モデルを選択してAPIを作成
   - 作成したAPIの「起動」ボタンをクリック

3. **テストページに移動**
   - 「API一覧」ページで、テストしたいAPIの「テスト」ボタンをクリック
   - または、メニューから「LLMテスト」を選択

4. **自動テストを実行**
   - テストページ上部の「LLM実行テスト」セクションで「テスト実行」ボタンをクリック
   - デフォルトのテストメッセージで自動的にテストが実行されます
   - カスタムメッセージを入力することも可能です

5. **結果の確認**
   - 成功率、平均応答時間、各テストの結果が表示されます
   - 成功したテストは緑色、失敗したテストは赤色で表示されます

### 方法2: 手動チャットテスト

1. **テストページに移動**（上記の手順1-3を参照）

2. **メッセージを送信**
   - チャット入力欄にメッセージを入力
   - 「送信」ボタンをクリック、またはEnterキーを押す

3. **応答を確認**
   - LLMからの応答が表示されます
   - トークン数や応答時間も確認できます

### 方法3: Jestテストを実行

```bash
# APIテストを実行
npm test -- tests/api/chat-api.test.ts

# すべてのテストを実行
npm test
```

**注意**: JestテストはTauriアプリケーションが起動している必要があります。

### 方法4: コマンドラインスクリプトを使用

```bash
# 環境変数を設定してテストスクリプトを実行
API_ENDPOINT=http://localhost:8080 API_KEY=your_api_key MODEL_NAME=llama3:8b node test-llm.js
```

## テスト項目

### 自動テストで実行される項目

1. **基本的な応答テスト**
   - 「こんにちは」などの簡単なメッセージ
   - 応答が返ってくることを確認

2. **コード生成テスト**
   - 「PythonでHello Worldを書いて」などのコード生成リクエスト
   - コードが正しく生成されることを確認

3. **計算テスト**
   - 「1+1は？」などの計算リクエスト
   - 正しい計算結果が返ってくることを確認

4. **知識テスト**
   - 「日本の首都はどこですか？」などの知識問題
   - 正しい情報が返ってくることを確認

### テスト結果の評価

- **成功率**: すべてのテストが成功した場合、100%と表示されます
- **平均応答時間**: すべてのテストの平均応答時間が表示されます
- **個別結果**: 各テストの詳細な結果が表示されます

## トラブルシューティング

### APIが起動しない場合

1. Ollamaが起動しているか確認
   ```bash
   ollama ps
   ```

2. ポートが使用中でないか確認
   - 別のアプリケーションが同じポートを使用していないか確認

3. モデルがインストールされているか確認
   ```bash
   ollama list
   ```

### テストが失敗する場合

1. **接続エラー**
   - APIが起動しているか確認
   - エンドポイントが正しいか確認

2. **認証エラー**
   - APIキーが正しく設定されているか確認
   - 認証が有効になっているか確認

3. **タイムアウトエラー**
   - モデルが大きすぎる場合、応答に時間がかかることがあります
   - タイムアウト時間を延長することを検討してください

4. **無効なレスポンス**
   - APIサーバーのログを確認
   - モデルが正しく動作しているか確認

## テスト結果のログ

テスト結果は以下の場所に記録されます：

- **アプリケーションログ**: `logs/app.log`（設定されている場合）
- **コンソールログ**: 開発者ツールのコンソール
- **テスト結果表示**: テストページの「LLM実行テスト」セクション

## カスタムテストメッセージ

テストページの「カスタムテストメッセージ」欄に、1行に1つずつテストメッセージを入力することで、独自のテストケースを実行できます。

例：
```
こんにちは
PythonでHello Worldを書いて
1+1は？
日本の首都はどこですか？
```

## 注意事項

- テスト実行中は、APIサーバーに負荷がかかります
- 複数のテストを同時に実行しないでください
- テスト間には1秒の待機時間が設けられています（レート制限対策）

